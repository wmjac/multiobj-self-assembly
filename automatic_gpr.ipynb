{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import pearsonr, norm\n",
    "from scipy.optimize import minimize, fmin_l_bfgs_b, shgo\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a0</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>N*</th>\n",
       "      <th>Frac</th>\n",
       "      <th>err</th>\n",
       "      <th>stan</th>\n",
       "      <th>k80</th>\n",
       "      <th>k80_err</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nmo</th>\n",
       "      <td>5.89000</td>\n",
       "      <td>17.90000</td>\n",
       "      <td>2.49000</td>\n",
       "      <td>1.82300</td>\n",
       "      <td>105.8</td>\n",
       "      <td>27.552083</td>\n",
       "      <td>44.34410</td>\n",
       "      <td>3.651754</td>\n",
       "      <td>-12.45200</td>\n",
       "      <td>0.1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>5.77569</td>\n",
       "      <td>24.25110</td>\n",
       "      <td>3.57752</td>\n",
       "      <td>1.82108</td>\n",
       "      <td>156.5</td>\n",
       "      <td>40.755208</td>\n",
       "      <td>93.28360</td>\n",
       "      <td>7.681942</td>\n",
       "      <td>-5.44875</td>\n",
       "      <td>0.0722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P8</th>\n",
       "      <td>5.50000</td>\n",
       "      <td>15.49900</td>\n",
       "      <td>3.33500</td>\n",
       "      <td>1.82300</td>\n",
       "      <td>339.9</td>\n",
       "      <td>88.515625</td>\n",
       "      <td>41.28610</td>\n",
       "      <td>3.399927</td>\n",
       "      <td>-5.79103</td>\n",
       "      <td>0.0637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nP2</th>\n",
       "      <td>5.51190</td>\n",
       "      <td>15.74929</td>\n",
       "      <td>3.33840</td>\n",
       "      <td>1.82430</td>\n",
       "      <td>358.9</td>\n",
       "      <td>93.463542</td>\n",
       "      <td>18.83530</td>\n",
       "      <td>1.551094</td>\n",
       "      <td>-5.54803</td>\n",
       "      <td>0.0624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nP6</th>\n",
       "      <td>5.51140</td>\n",
       "      <td>15.77750</td>\n",
       "      <td>3.34330</td>\n",
       "      <td>1.82560</td>\n",
       "      <td>301.9</td>\n",
       "      <td>78.619792</td>\n",
       "      <td>56.07820</td>\n",
       "      <td>4.618062</td>\n",
       "      <td>-5.32668</td>\n",
       "      <td>0.0327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9</th>\n",
       "      <td>5.51500</td>\n",
       "      <td>15.66000</td>\n",
       "      <td>3.34560</td>\n",
       "      <td>1.82560</td>\n",
       "      <td>287.0</td>\n",
       "      <td>74.739583</td>\n",
       "      <td>72.55500</td>\n",
       "      <td>5.974933</td>\n",
       "      <td>-5.29832</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r1</th>\n",
       "      <td>5.50000</td>\n",
       "      <td>11.69980</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82400</td>\n",
       "      <td>13.8</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>8.56089</td>\n",
       "      <td>0.704993</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>5.51170</td>\n",
       "      <td>15.58190</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82430</td>\n",
       "      <td>16.3</td>\n",
       "      <td>4.244792</td>\n",
       "      <td>7.58727</td>\n",
       "      <td>0.624815</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r3</th>\n",
       "      <td>5.50000</td>\n",
       "      <td>14.64490</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82400</td>\n",
       "      <td>12.5</td>\n",
       "      <td>3.255208</td>\n",
       "      <td>4.76678</td>\n",
       "      <td>0.392546</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r4</th>\n",
       "      <td>5.51170</td>\n",
       "      <td>15.67780</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82430</td>\n",
       "      <td>18.6</td>\n",
       "      <td>4.843750</td>\n",
       "      <td>7.07421</td>\n",
       "      <td>0.582564</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r5</th>\n",
       "      <td>5.50000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>4.99990</td>\n",
       "      <td>1.82400</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.864583</td>\n",
       "      <td>4.76095</td>\n",
       "      <td>0.392066</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r6</th>\n",
       "      <td>5.50000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.83000</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.26491</td>\n",
       "      <td>0.104166</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r7</th>\n",
       "      <td>5.51060</td>\n",
       "      <td>15.67350</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.82410</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.68655</td>\n",
       "      <td>0.138888</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r8</th>\n",
       "      <td>5.51160</td>\n",
       "      <td>15.72880</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82420</td>\n",
       "      <td>11.3</td>\n",
       "      <td>2.942708</td>\n",
       "      <td>3.86005</td>\n",
       "      <td>0.317877</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r9</th>\n",
       "      <td>5.51066</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82410</td>\n",
       "      <td>14.4</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>7.57481</td>\n",
       "      <td>0.623789</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r10</th>\n",
       "      <td>5.51114</td>\n",
       "      <td>15.74790</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.82419</td>\n",
       "      <td>11.8</td>\n",
       "      <td>3.072917</td>\n",
       "      <td>4.82586</td>\n",
       "      <td>0.397411</td>\n",
       "      <td>-20.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a0        a1       a2       a3     N*       Frac       err  \\\n",
       "lab                                                                    \n",
       "nmo  5.89000  17.90000  2.49000  1.82300  105.8  27.552083  44.34410   \n",
       "P2   5.77569  24.25110  3.57752  1.82108  156.5  40.755208  93.28360   \n",
       "P8   5.50000  15.49900  3.33500  1.82300  339.9  88.515625  41.28610   \n",
       "nP2  5.51190  15.74929  3.33840  1.82430  358.9  93.463542  18.83530   \n",
       "nP6  5.51140  15.77750  3.34330  1.82560  301.9  78.619792  56.07820   \n",
       "P9   5.51500  15.66000  3.34560  1.82560  287.0  74.739583  72.55500   \n",
       "r1   5.50000  11.69980  5.00000  1.82400   13.8   3.593750   8.56089   \n",
       "r2   5.51170  15.58190  5.00000  1.82430   16.3   4.244792   7.58727   \n",
       "r3   5.50000  14.64490  5.00000  1.82400   12.5   3.255208   4.76678   \n",
       "r4   5.51170  15.67780  5.00000  1.82430   18.6   4.843750   7.07421   \n",
       "r5   5.50000  10.00000  4.99990  1.82400   11.0   2.864583   4.76095   \n",
       "r6   5.50000  10.00000  1.00000  1.83000    3.6   0.937500   1.26491   \n",
       "r7   5.51060  15.67350  1.00000  1.82410    0.8   0.208333   1.68655   \n",
       "r8   5.51160  15.72880  5.00000  1.82420   11.3   2.942708   3.86005   \n",
       "r9   5.51066  30.00000  5.00000  1.82410   14.4   3.750000   7.57481   \n",
       "r10  5.51114  15.74790  5.00000  1.82419   11.8   3.072917   4.82586   \n",
       "\n",
       "         stan       k80  k80_err  \n",
       "lab                               \n",
       "nmo  3.651754 -12.45200   0.1188  \n",
       "P2   7.681942  -5.44875   0.0722  \n",
       "P8   3.399927  -5.79103   0.0637  \n",
       "nP2  1.551094  -5.54803   0.0624  \n",
       "nP6  4.618062  -5.32668   0.0327  \n",
       "P9   5.974933  -5.29832   0.0000  \n",
       "r1   0.704993 -20.00000   0.0000  \n",
       "r2   0.624815 -20.00000   0.0000  \n",
       "r3   0.392546 -20.00000   0.0000  \n",
       "r4   0.582564 -20.00000   0.0000  \n",
       "r5   0.392066 -20.00000   0.0000  \n",
       "r6   0.104166 -20.00000   0.0000  \n",
       "r7   0.138888 -20.00000   0.0000  \n",
       "r8   0.317877 -20.00000   0.0000  \n",
       "r9   0.623789 -20.00000   0.0000  \n",
       "r10  0.397411 -20.00000   0.0000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname=\"../wall/gpr/generation_2_rw.xlsx\" # SPECIFY FILE TO BE READ..COLUMNS: 'lab' (label), 'a_i' (design paramters), 'Frac'(Yf), 'stan' (std err of Yf), 'k80' (K), 'k80_err' (K error)\n",
    "gen = pd.read_excel(fname,index_col='lab')\n",
    "n_des_param = 4 # number of design paramters\n",
    "opt_global = False # Flag to choose between global and local optimization for Expected Improvement\n",
    "local_method = \"nelder-mead\" # or \"BFGS\" or \"L-BFGS-B\"\n",
    "nmot = 27.552 #Y^f NMO\n",
    "nmok = -12.452 #K NMO\n",
    "b = [[5.,6.5],[1,100],[1,5],[1.8,1.83]] # Bounds over design parameters\n",
    "bounds = np.array(b)\n",
    "gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR Thermodynamic - Kernel Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged kernel params:  [0.9692 0.972  1.0392 1.0392 1.0392]\n",
      "Converged test data pearson correlation =  0.9020391135500766\n",
      "Total run time = 479.02845311164856 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "niter = 20 # number of outer optimizations over kernel parameters; need to vary this based on dataset\n",
    "kernel_nu = 0.5 # need to vary between 0.5,1.5,2.5 based on data\n",
    "\n",
    "#** function to calculate averaged correlation between training and test datasets **\n",
    "def obj(k): \n",
    "    corr_tr=0\n",
    "    corr_tt=0\n",
    "    samp_iter=0\n",
    "    while samp_iter < 100:\n",
    "        samp_iter = samp_iter + 1\n",
    "        sample = gen.sample(frac=0.5)\n",
    "        xtrain = sample[['a0','a1','a2','a3']].to_numpy() #need to specify column names of design parameters\n",
    "        xtrain.reshape(-1,n_des_param)\n",
    "        ytrain = sample['Frac'].to_numpy()\n",
    "        ytrain_err = sample['stan'].to_numpy()\n",
    "        ytrain = ytrain.ravel()\n",
    "        kernel = C(k[0],(1e-2,1e2)) * Matern((k[1:]), length_scale_bounds=(1e-3,1e3),nu=kernel_nu)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        gp_therm = GaussianProcessRegressor(kernel=kernel, alpha=ytrain_err, n_restarts_optimizer=5)\n",
    "        gp_therm.fit(xtrain,ytrain)\n",
    "        xtest=[[0]*n_des_param]\n",
    "        sim=[0]\n",
    "        sim_err = [0]\n",
    "        for index,row in gen.iterrows():\n",
    "            if index in sample.index:\n",
    "                continue\n",
    "            else:\n",
    "                r=row[['a0','a1','a2','a3']].to_numpy() # again need to specify column names\n",
    "                xtest = np.vstack([xtest,r])\n",
    "                sim.append(row['Frac'])\n",
    "                sim_err.append(row['stan'])\n",
    "        y_pred, sigma = gp_therm.predict(xtest, return_std=True)\n",
    "        y_pred_cont, sig_cont = gp_therm.predict(xtrain, return_std=True)\n",
    "        sigma = [s/2 for s in sigma]\n",
    "        cptr,_ = pearsonr(y_pred_cont,ytrain)\n",
    "        cptt,_ = pearsonr(y_pred,sim)\n",
    "        corr_tr = corr_tr + cptr\n",
    "        corr_tt = corr_tt + cptt\n",
    "    corr_tr = corr_tr/samp_iter\n",
    "    corr_tt = corr_tt/samp_iter\n",
    "    return -corr_tt\n",
    "\n",
    "k = np.array([1]*(n_des_param+1)) #initial kernel parameters\n",
    "result = minimize(obj,k,method='nelder-mead',options={\"maxiter\":niter})\n",
    "solution = result['x'] #optimized kernel parameters\n",
    "evaluation = -obj(solution)\n",
    "print(\"Converged kernel params: \", solution)\n",
    "print(\"Converged test data pearson correlation = \", evaluation)\n",
    "\n",
    "xtr = gen[['a0','a1','a2','a3']].to_numpy()\n",
    "ytr = gen['Frac'].to_numpy()\n",
    "ytr_err = gen['stan'].to_numpy()\n",
    "ytr = ytr.ravel()\n",
    "conv_kernel = C(solution[0],(1e-2,1e2)) * Matern((solution[1:]), length_scale_bounds=(1e-3,1e3),nu=kernel_nu)\n",
    "gpr_therm = GaussianProcessRegressor(kernel=conv_kernel,alpha=ytr_err,n_restarts_optimizer=10)\n",
    "gpr_therm.fit(xtr,ytr)\n",
    "print(\"Total run time = {} seconds\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR Kinetic - Kernel Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged kernel params:  [1.05340271 0.99791069 0.97187546 1.04164243 0.98382888]\n",
      "Converged test data pearson correlation =  0.7472860672999964\n",
      "Total run time = 470.00868487358093 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "niter = 20 #number of outer optimizations over kernel parameters; need to vary this based on dataset\n",
    "kernel_nu = 0.5 # need to vary between 0.5,1.5,2.5 based on data\n",
    "\n",
    "#** function to calculate averaged correlation between training and test datasets **\n",
    "def obj(k):\n",
    "    corr_tr=0\n",
    "    corr_tt=0\n",
    "    samp_iter=0\n",
    "    while samp_iter < 100:\n",
    "        samp_iter = samp_iter + 1\n",
    "        sample = gen.sample(frac=0.5)\n",
    "        xtrain = sample[['a0','a1','a2','a3']].to_numpy() #need to specify column names of design parameters\n",
    "        xtrain.reshape(-1,n_des_param)\n",
    "        ytrain = sample['k80'].to_numpy()\n",
    "        ytrain_err = sample['k80_err'].to_numpy()\n",
    "        ytrain = ytrain.ravel()\n",
    "        kernel = C(k[0],(1e-2,1e2)) * Matern((k[1:]), length_scale_bounds=(1e-3,1e3),nu=kernel_nu)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        gp_kin = GaussianProcessRegressor(kernel=kernel, alpha=ytrain_err, n_restarts_optimizer=5)\n",
    "        gp_kin.fit(xtrain,ytrain)\n",
    "        xtest=[[0]*n_des_param]\n",
    "        sim=[0]\n",
    "        sim_err = [0]\n",
    "        for index,row in gen.iterrows():\n",
    "            if index in sample.index:\n",
    "                continue\n",
    "            else:\n",
    "                r=row[['a0','a1','a2','a3']].to_numpy() # again need to specify column names\n",
    "                xtest = np.vstack([xtest,r])\n",
    "                sim.append(row['k80'])\n",
    "                sim_err.append(row['k80_err'])\n",
    "        y_pred, sigma = gp_kin.predict(xtest, return_std=True)\n",
    "        y_pred_cont, sig_cont = gp_kin.predict(xtrain, return_std=True)\n",
    "        sigma = [s/2 for s in sigma]\n",
    "        cptr,_ = pearsonr(y_pred_cont,ytrain)\n",
    "        cptt,_ = pearsonr(y_pred,sim)\n",
    "        corr_tr = corr_tr + cptr\n",
    "        corr_tt = corr_tt + cptt\n",
    "    corr_tr = corr_tr/samp_iter\n",
    "    corr_tt = corr_tt/samp_iter\n",
    "    return -corr_tt\n",
    "\n",
    "k = np.array([1]*(n_des_param+1)) # initial kernel paramteres\n",
    "result = minimize(obj,k,method='nelder-mead',options={\"maxiter\":niter})\n",
    "solution = result['x'] #optimized kernel parameters\n",
    "evaluation = -obj(solution)\n",
    "print(\"Converged kernel params: \", solution)\n",
    "print(\"Converged test data pearson correlation = \", evaluation)\n",
    "\n",
    "xtr = gen[['a0','a1','a2','a3']].to_numpy()\n",
    "ytr = gen['k80'].to_numpy()\n",
    "ytr_err = gen['k80_err'].to_numpy()\n",
    "ytr = ytr.ravel()\n",
    "conv_kernel = C(solution[0],(1e-2,1e2)) * Matern((solution[1:]), length_scale_bounds=(1e-3,1e3),nu=kernel_nu)\n",
    "gpr_kin = GaussianProcessRegressor(kernel=conv_kernel,alpha=ytr_err,n_restarts_optimizer=10)\n",
    "gpr_kin.fit(xtr,ytr)\n",
    "print(\"Total run time = {} seconds\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximize Expected Improvement, Propose design vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0: (1) des. param. = [ 6.5        95.94170941  3.43912993  1.8       ], Yf_pred = [43.52328371], sigmaY = [8.49703667], K_pred = [-6.46688172], sigmaK = [2.73981999], EI = [0.60668622]\n",
      "lambda = 0.1: (2) des. param. = [  5.         100.           1.99077749   1.8       ], Yf_pred = [0.48746336], sigmaY = [9.99837879], K_pred = [-14.3880469], sigmaK = [3.81951083], EI = [6.76083375e-05]\n",
      "lambda = 0.2: (3) des. param. = [ 5.00023588 45.27424204  3.33629905  1.81898669], Yf_pred = [86.9531505], sigmaY = [2.43551256], K_pred = [-5.88017444], sigmaK = [1.78717066], EI = [0.35920978]\n",
      "lambda = 0.3: (4) des. param. = [ 6.10233615 30.0329452   3.33729786  1.83      ], Yf_pred = [88.80177241], sigmaY = [1.84665448], K_pred = [-5.67286011], sigmaK = [1.26861398], EI = [0.29971572]\n",
      "lambda = 0.4: (5) des. param. = [ 6.5        22.34722852  3.33780129  1.8       ], Yf_pred = [89.81210532], sigmaY = [1.51331924], K_pred = [-5.58277144], sigmaK = [0.86944997], EI = [0.27659321]\n",
      "lambda = 0.5: (6) des. param. = [  5.01844433 100.           3.33300172   1.83      ], Yf_pred = [81.33207491], sigmaY = [4.00068482], K_pred = [-6.72462786], sigmaK = [2.86873597], EI = [0.09069321]\n",
      "lambda = 0.6: (7) des. param. = [ 6.5        16.67449336  3.33824678  1.83      ], Yf_pred = [90.57130601], sigmaY = [1.19996549], K_pred = [-5.55273427], sigmaK = [0.4002078], EI = [0.29274033]\n",
      "lambda = 0.7: (8) des. param. = [  5.         100.           2.34240707   1.83      ], Yf_pred = [8.12851926], sigmaY = [9.53838957], K_pred = [-12.73950582], sigmaK = [3.15898037], EI = [1.65674221e-15]\n",
      "lambda = 0.8: (9) des. param. = [6.5        1.         2.67741301 1.83      ], Yf_pred = [6.35856866], sigmaY = [9.7573292], K_pred = [-10.88297585], sigmaK = [2.45624758], EI = [6.63637111e-17]\n",
      "lambda = 0.9: (10) des. param. = [ 6.5        15.78307457  3.33827982  1.8       ], Yf_pred = [90.62497462], sigmaY = [1.17314991], K_pred = [-5.56111482], sigmaK = [0.34774085], EI = [0.36874597]\n",
      "lambda = 1: (11) des. param. = [ 5.         15.75012275  3.33833554  1.82580021], Yf_pred = [90.69878733], sigmaY = [1.13261857], K_pred = [-5.56141658], sigmaK = [0.28116369], EI = [0.41248725]\n",
      "Total number of predicted potentials = 11\n",
      "Average Expected Improvement = [0.24608539]\n"
     ]
    }
   ],
   "source": [
    "ll = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] #lambda \n",
    "xkr = gen[['a0','a1','a2','a3']].to_numpy()\n",
    "yk = gen['k80'].to_numpy()\n",
    "yt = gen['Frac'].to_numpy()\n",
    "\n",
    "def func(x,l):\n",
    "    gt, sigt = gpr_therm.predict(x, return_std=True)\n",
    "    gk, sigk = gpr_kin.predict(x, return_std=True)\n",
    "    mean = l*gt + (1-l)*gk\n",
    "    sigma = l*sigt + (1-l)*sigk\n",
    "    return mean, sigma\n",
    "\n",
    "def expI(x, l, f, evaluated_loss, n_params=n_des_param):\n",
    "    x_to_predict = x.reshape(-1,n_params)\n",
    "    mu, sigma = f(x_to_predict,l)\n",
    "    loss_optimum = np.max(evaluated_loss)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = (mu-loss_optimum)/sigma\n",
    "        expected_improvement = (mu-loss_optimum) * norm.cdf(Z) + sigma*norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "    return expected_improvement\n",
    "\n",
    "count=0\n",
    "ei=0\n",
    "for i in range(len(ll)):\n",
    "    ev_loss = ll[i]*gpr_therm.predict(xkr) + (1-ll[i])*gpr_kin.predict(xkr)\n",
    "    def f_to_call(x):\n",
    "        res = expI(x,ll[i],func,ev_loss,n_params=n_des_param)\n",
    "        return -res\n",
    "    for k in range(1): # Vary this to control number of predicted potentials for each \\lambda\n",
    "        x_random = np.random.uniform(bounds[:,0],bounds[:,1], size=(1,n_des_param))\n",
    "        xr = [[x_random]]\n",
    "        if (opt_global==True):\n",
    "            result = shgo(f_to_call,bounds)\n",
    "        else:\n",
    "            result = minimize(f_to_call,xr,method=local_method,bounds=bounds)\n",
    "        gt,st = gpr_therm.predict([result['x']],return_std=True)\n",
    "        gk,sk = gpr_kin.predict([result['x']],return_std=True)\n",
    "        if ((gt>nmot and gk>nmok)or ((st>2 and st<6) or (sk>2 and sk<6))):\n",
    "            exp_imp = -f_to_call(result['x'])\n",
    "            print(\"lambda = {}: ({}) des. param. = {}, Yf_pred = {}, sigmaY = {}, K_pred = {}, sigmaK = {}, EI = {}\".format(ll[i],count+1,result['x'],gt,st,gk,sk,exp_imp))\n",
    "            ei = ei + exp_imp\n",
    "            count+=1\n",
    "\n",
    "avg_ei = ei/count\n",
    "print(\"Total number of predicted potentials = {}\".format(count))\n",
    "print(\"Average Expected Improvement = {}\".format(avg_ei))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
